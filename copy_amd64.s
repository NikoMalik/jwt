#include "textflag.h"




DATA MASK1<>+0x00(SB)/8, $2
DATA MASK1<>+0x08(SB)/8, $6
DATA MASK1<>+0x10(SB)/8, $10
DATA MASK1<>+0x18(SB)/8, $14
DATA MASK1<>+0x20(SB)/8, $0x80
DATA MASK1<>+0x28(SB)/8, $0x80
DATA MASK1<>+0x30(SB)/8, $0x80
DATA MASK1<>+0x38(SB)/8, $0x80
DATA MASK1<>+0x40(SB)/8, $0x80 
DATA MASK1 <>+0x48(SB)/8, $0x80
DATA MASK1<>+0x50(SB)/8, $0x80
DATA MASK1<>+0x58(SB)/8, $0x80
DATA MASK1<>+0x60(SB)/8, $0x80
DATA MASK1<>+0x68(SB)/8, $0x80
DATA MASK1<>+0x70(SB)/8, $0x80
DATA MASK1<>+0x78(SB)/8, $0x80
DATA MASK1<>+0x80(SB)/8, $2 
DATA MASK1<>+0x88(SB)/8, $6
DATA MASK1<>+0x90(SB)/8, $10
DATA MASK1<>+0x98(SB)/8, $14
DATA MASK1<>+0xA0(SB)/8, $0x80
DATA MASK1<>+0xA8(SB)/8, $0x80
DATA MASK1<>+0xB0(SB)/8, $0x80
DATA MASK1<>+0xB8(SB)/8, $0x80
DATA MASK1<>+0xC0(SB)/8, $0x80
DATA MASK1<>+0xC8(SB)/8, $0x80
DATA MASK1<>+0xD0(SB)/8, $0x80
DATA MASK1<>+0xD8(SB)/8, $0x80
DATA MASK1<>+0xE0(SB)/8, $0x80
DATA MASK1<>+0xE8(SB)/8, $0x80
DATA MASK1<>+0xF0(SB)/8, $0x80
DATA MASK1<>+0xF8(SB)/8, $0x80


GLOBL MASK1(SB), RODATA, $32



DATA MASK2<>+0x00(SB)/8, $0x80
DATA MASK2<>+0x08(SB)/8, $0x80
DATA MASK2<>+0x10(SB)/8, $0x80
DATA MASK2<>+0x18(SB)/8, $0x80
DATA MASK2<>+0x20(SB)/8, $2
DATA MASK2<>+0x28(SB)/8, $6
DATA MASK2<>+0x30(SB)/8, $0xA
DATA MASK2<>+0x38(SB)/8, $0xE
DATA MASK2<>+0x40(SB)/8, $0x80
DATA MASK2<>+0x48(SB)/8, $0x80
DATA MASK2<>+0x50(SB)/8, $0x80
DATA MASK2<>+0x58(SB)/8, $0x80
DATA MASK2<>+0x60(SB)/8, $0x80
DATA MASK2<>+0x68(SB)/8, $0x80
DATA MASK2<>+0x70(SB)/8, $0x80
DATA MASK2<>+0x78(SB)/8, $0x80
DATA MASK2<>+0x80(SB)/8, $0x80
DATA MASK2<>+0x88(SB)/8, $0x80
DATA MASK2<>+0x90(SB)/8, $0x80
DATA MASK2<>+0x98(SB)/8, $0x80
DATA MASK2<>+0xA0(SB)/8, $2
DATA MASK2<>+0xA8(SB)/8, $6
DATA MASK2<>+0xB0(SB)/8, $10
DATA MASK2<>+0xB8(SB)/8, $14 
DATA MASK2<>+0xC0(SB)/8, $0x80
DATA MASK2<>+0xC8(SB)/8, $0x80
DATA MASK2<>+0xD0(SB)/8, $0x80
DATA MASK2<>+0xD8(SB)/8, $0x80
DATA MASK2<>+0xE0(SB)/8, $0x80
DATA MASK2<>+0xE8(SB)/8, $0x80
DATA MASK2<>+0xF0(SB)/8, $0x80
DATA MASK2<>+0xF8(SB)/8, $0x80

GLOBL MASK2(SB), RODATA, $32

DATA MASK3<>+0x00(SB)/8, $0x80
DATA MASK3<>+0x08(SB)/8, $0x80
DATA MASK3<>+0x10(SB)/8, $0x80
DATA MASK3<>+0x18(SB)/8, $0x80
DATA MASK3<>+0x20(SB)/8, $0x80
DATA MASK3<>+0x28(SB)/8, $0x80
DATA MASK3<>+0x30(SB)/8, $0x80
DATA MASK3<>+0x38(SB)/8, $0x80
DATA MASK3<>+0x40(SB)/8, $2
DATA MASK3<>+0x48(SB)/8, $6
DATA MASK3<>+0x50(SB)/8, $0xA
DATA MASK3<>+0x58(SB)/8, $0xE
DATA MASK3<>+0x60(SB)/8, $0x80
DATA MASK3<>+0x68(SB)/8, $0x80
DATA MASK3<>+0x70(SB)/8, $0x80
DATA MASK3<>+0x78(SB)/8, $0x80
DATA MASK3<>+0x80(SB)/8, $0x80
DATA MASK3<>+0x88(SB)/8, $0x80
DATA MASK3<>+0x90(SB)/8, $0x80
DATA MASK3<>+0x98(SB)/8, $0x80
DATA MASK3<>+0xA0(SB)/8, $0x80
DATA MASK3<>+0xA8(SB)/8, $0x80
DATA MASK3<>+0xB0(SB)/8, $0x80
DATA MASK3<>+0xB8(SB)/8, $0x80
DATA MASK3<>+0xC0(SB)/8, $2
DATA MASK3<>+0xC8(SB)/8, $6
DATA MASK3<>+0xD0(SB)/8, $0xA
DATA MASK3<>+0xD8(SB)/8, $0xE
DATA MASK3<>+0xE0(SB)/8, $0x80
DATA MASK3<>+0xE8(SB)/8, $0x80
DATA MASK3<>+0xF0(SB)/8, $0x80
DATA MASK3<>+0xF8(SB)/8, $0x80

GLOBL MASK3(SB), RODATA, $32

DATA MASK4<>+0x00(SB)/8, $0x80
DATA MASK4<>+0x08(SB)/8, $0x80
DATA MASK4<>+0x10(SB)/8, $0x80
DATA MASK4<>+0x18(SB)/8, $0x80
DATA MASK4<>+0x20(SB)/8, $0x80
DATA MASK4<>+0x28(SB)/8, $0x80
DATA MASK4<>+0x30(SB)/8, $0x80
DATA MASK4<>+0x38(SB)/8, $0x80
DATA MASK4<>+0x40(SB)/8, $0x80
DATA MASK4<>+0x48(SB)/8, $0x80
DATA MASK4<>+0x50(SB)/8, $0x80
DATA MASK4<>+0x58(SB)/8, $0x80
DATA MASK4<>+0x60(SB)/8, $2
DATA MASK4<>+0x68(SB)/8, $6
DATA MASK4<>+0x70(SB)/8, $0xA
DATA MASK4<>+0x78(SB)/8, $0xE
DATA MASK4<>+0x80(SB)/8, $0x80
DATA MASK4<>+0x88(SB)/8, $0x80
DATA MASK4<>+0x90(SB)/8, $0x80
DATA MASK4<>+0x98(SB)/8, $0x80
DATA MASK4<>+0xA0(SB)/8, $0x80
DATA MASK4<>+0xA8(SB)/8, $0x80
DATA MASK4<>+0xB0(SB)/8, $0x80
DATA MASK4<>+0xB8(SB)/8, $0x80
DATA MASK4<>+0xC0(SB)/8, $0x80
DATA MASK4<>+0xC8(SB)/8, $0x80
DATA MASK4<>+0xD0(SB)/8, $0x80
DATA MASK4<>+0xD8(SB)/8, $0x80
DATA MASK4<>+0xE0(SB)/8, $2
DATA MASK4<>+0xE8(SB)/8, $6
DATA MASK4<>+0xF0(SB)/8, $0xA
DATA MASK4<>+0xF8(SB)/8, $0xE




GLOBL MASK4(SB), RODATA, $32

DATA P_MASK<>+0x00(SB)/8, $0
DATA P_MASK<>+0x08(SB)/8, $4
DATA P_MASK<>+0x10(SB)/8, $1
DATA P_MASK<>+0x18(SB)/8, $5
DATA P_MASK<>+0x20(SB)/8, $2
DATA P_MASK<>+0x28(SB)/8, $6
DATA P_MASK<>+0x30(SB)/8, $3
DATA P_MASK<>+0x38(SB)/8, $7
// 8
GLOBL P_Mask(SB), RODATA, $32




TEXT Â·_copy_(SB), $0-48
    MOVQ    src_1+0(FP), AX    // Load src_1 pointer into AX register
    MOVQ    src_2+24(FP), CX   // Load src_2 pointer into CX register
    MOVQ    CX, DI

    
    VMOVDQU MASK1(SB), Y10     // Load MASK1 into Y10 register
    VMOVDQU MASK2(SB), Y11     // Load MASK2 into Y11 register
    VMOVDQU MASK3(SB), Y12     // Load MASK3 into Y12 register
    VMOVDQU MASK4(SB), Y13     // Load MASK4 into Y13 register
    VMOVDQU P_Mask(SB), Y14    // Load P_Mask into Y14 register

LOOP:
    VMOVDQU (AX), Y0           // Load 128 bytes from src_1 to Y0
    VMOVDQU 32(AX), Y1         // Load next 128 bytes from src_1 to Y1
    VMOVDQU 64(AX), Y2         // Load next 128 bytes from src_1 to Y2
    VMOVDQU 96(AX), Y3         // Load next 128 bytes from src_1 to Y3

    VPSHUFB Y10, Y0, Y0        // Apply MASK1 to Y0
    VPSHUFB Y11, Y1, Y1        // Apply MASK2 to Y1
    VPSHUFB Y12, Y2, Y2        // Apply MASK3 to Y2
    VPSHUFB Y13, Y3, Y3        // Apply MASK4 to Y3

    VPOR Y0, Y1, Y0            // Perform bitwise OR between Y0 and Y1
    VPOR Y2, Y3, Y2            // Perform bitwise OR between Y2 and Y3
    VPOR Y0, Y2, Y0            // Perform bitwise OR between the results

    VPERMD Y0, Y14, Y0         // Perform a permutation based on P_Mask

    VMOVDQU Y0, (CX)           // Store the result into src_2 (CX)

    ADDQ    $32, CX            // Move to the next 32-byte block in src_2
    ADDQ    $128, AX           // Move to the next 128-byte block in src_1

    CMPQ    DI, CX             // Compare src_2 pointer (CX) with DI
    JGT     LOOP               // Jump if CX is greater than DI (loop condition)
    
    RET                         // Return from the function



